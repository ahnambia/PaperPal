model:
  name: "facebook/bart-base"
  cache_dir: "./models/cache"

tokenizer:
  max_input_length: 512
  max_target_length: 128

training:
  output_dir: "./models/checkpoints"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 3.0e-5
  
  evaluation_strategy: "no"
  save_strategy: "epoch"
  save_total_limit: 2
  load_best_model_at_end: false  # Can't load best without eval!
  
  logging_steps: 1
  report_to: ["tensorboard"]
  
  dataloader_num_workers: 0

data:
  num_workers: 0

seed: 42
