# ğŸ“„ PaperPal â€” Research Paper Summarizer & Insight Extractor (HF + PyTorch)

**PaperPal** is an end-to-end NLP system that downloads research papers from **arXiv**, summarizes them using a fine-tuned transformer model, and extracts key insights such as methods and results.  
The project culminates in an interactive **Streamlit UI**, served via **FastAPI**, and exported as a **Hugging Face Space** demo.

---

## ğŸ§  Tech Stack
- **Core:** Python â€¢ Hugging Face Transformers â€¢ PyTorch  
- **Retrieval:** FAISS â€¢ RAG (Retrieval-Augmented Generation)  
- **Frontend & Serving:** Streamlit â€¢ FastAPI  
- **Utilities:** pandas â€¢ scikit-learn â€¢ dotenv â€¢ arxiv API

---

## âœ… Progress (Completed â€“ Day 1)

### ğŸ“¦ Repository Scaffold
- Organized `src/`, `scripts/`, and `data/` directories for clean modular development.  
- Added configuration system (`Config` class + `.env`) to manage categories, keywords, and dataset splits.

### ğŸ” arXiv Downloader
- Built `arxiv_client.py` for reliable API queries with paging and polite rate-limiting.  
- Script `scripts/download_arxiv.py` saves timestamped raw metadata â†’ `data/raw/arxiv_<timestamp>.jsonl`.

### ğŸ§¹ Dataset Preparation
- Implemented text cleaning (remove LaTeX/citations).  
- Added filtering (min abstract length + date range support).  
- Deduplicated papers by title and generated **train / val / test** splits.  
- Outputs both `.jsonl` and `.parquet` files under `data/processed/`.

---

## ğŸš€ Quickstart (So Far)

```bash
# 1. Create & activate environment
python -m venv .venv && source .venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure
cp .env.example .env  # edit categories or limits if desired

# 4. Download and prepare data
python scripts/download_arxiv.py --max-results 200
python scripts/prepare_dataset.py --min-abs-words 60 --val-size 0.1 --test-size 0.1

Outputs

Raw â†’ data/raw/arxiv_<timestamp>.jsonl

Processed â†’ data/processed/papers_{train,val,test}.{jsonl,parquet}

paperpal/
â”œâ”€ README.md
â”œâ”€ .gitignore
â”œâ”€ .env.example
â”œâ”€ requirements.txt
â”œâ”€ data/
â”‚  â”œâ”€ raw/         # arXiv JSONL snapshots (ignored by git)
â”‚  â”œâ”€ interim/     # intermediate files (ignored)
â”‚  â””â”€ processed/   # cleaned splits (ignored)
â”œâ”€ src/
â”‚  â””â”€ paperpal/
â”‚     â”œâ”€ __init__.py
â”‚     â”œâ”€ config.py
â”‚     â”œâ”€ arxiv_client.py
â”‚     â”œâ”€ data_prep.py
â”‚     â””â”€ utils/
â”‚        â”œâ”€ io.py
â”‚        â””â”€ text.py
â””â”€ scripts/
   â”œâ”€ download_arxiv.py
   â””â”€ prepare_dataset.py

Next Steps (Planned)
Day	Focus	Key Goals
Day 2	Fine-tuning Prep	Generate silver summaries, load tokenizer, and configure BART/T5 training
Day 3	RAG Integration	Build FAISS index + context retrieval pipeline
Day 4	Deployment & Demo	Streamlit UI + FastAPI API + Hugging Face Space export

Status: âœ… Day 1 Complete â€” Raw Downloader + Dataset Prep in place.
Next â†’ Day 2 Fine-Tuning Setup ğŸš€