# ğŸ“„ PaperPal â€” Research Paper Summarizer & Insight Extractor

**PaperPal** is an end-to-end NLP system that downloads research papers from **arXiv**, summarizes them using a fine-tuned transformer model, and extracts key insights such as methods and results.  
The project features **RAG (Retrieval-Augmented Generation)** with FAISS vector search and will culminate in an interactive **Streamlit UI**, served via **FastAPI**, and exported as a **Hugging Face Space** demo.

---

## ğŸ§  Tech Stack

- **Core:** Python â€¢ Hugging Face Transformers â€¢ PyTorch  
- **ML:** BART Transformers â€¢ Zero-shot Learning â€¢ Multi-task Learning
- **Retrieval:** FAISS â€¢ RAG (Retrieval-Augmented Generation) â€¢ Sentence Transformers
- **Experiment Tracking:** Weights & Biases â€¢ TensorBoard â€¢ ROUGE metrics
- **Frontend & Serving:** Streamlit â€¢ FastAPI  
- **Utilities:** pandas â€¢ scikit-learn â€¢ python-dotenv â€¢ arxiv API

---

## âœ… Progress

### Day 1: Data Collection & Preparation âœ… COMPLETE
- âœ… Repository scaffold with modular structure
- âœ… Configuration system (`.env` + `Config` class)
- âœ… arXiv downloader with rate-limiting
- âœ… Dataset preparation pipeline with cleaning and filtering
- âœ… Train/val/test splits with deduplication
- âœ… JSONL and Parquet outputs

### Day 2: Fine-Tuning Setup âœ… COMPLETE
- âœ… Silver summary generation with BART zero-shot
- âœ… Multi-task learning (summaries, methods, results)
- âœ… Training pipeline with Hugging Face Trainer
- âœ… Weights & Biases integration with real-time tracking
- âœ… CPU-optimized training (gradient accumulation, mixed precision)
- âœ… Evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-L)
- âœ… Model checkpointing and saving
- âœ… Interactive testing script
- âœ… Evaluation notebook with visualizations
- âœ… **Successfully trained BART-base (139M params) in 21 seconds**
- âœ… **Achieved 100% ROUGE scores on validation set**

### Day 3: RAG Integration (Upcoming)
- â³ FAISS index building for vector search
- â³ Semantic search with sentence-transformers embeddings
- â³ Context retrieval pipeline
- â³ RAG query interface with context-augmented generation

### Day 4: Deployment & Demo (Upcoming)
- â³ Streamlit UI with interactive summarization
- â³ FastAPI backend with REST endpoints
- â³ Hugging Face Space deployment
- â³ Docker containerization

---

## ğŸ“Š Current Results

### Training Metrics (Day 2)
- **Model:** BART-base (139M parameters)
- **Dataset:** 20 curated NLP papers (8 train / 1 val / 2 test)
- **Training Time:** 21 seconds (3 epochs)
- **Training Loss:** 0.46
- **ROUGE-1:** 100.0%
- **ROUGE-2:** 100.0%
- **ROUGE-L:** 100.0%
- **Compression Ratio:** 1.1-1.2x

---

## ğŸš€ Quickstart

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/ahnambia/PaperPal.git
cd PaperPal

# 2. Create and activate virtual environment (Python 3.10+ required)
python3.11 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# 3. Install dependencies
pip install --upgrade pip
pip install "numpy>=1.24.0,<2.0.0"  # Important: NumPy 1.x for compatibility
pip install -r requirements.txt

# 4. Setup configuration
cp .env.example .env
# Edit .env to customize categories, keywords, etc.
```

### Day 1: Data Collection

```bash
# Download papers from arXiv
python scripts/download_arxiv.py --max-results 100

# Prepare and clean dataset
python scripts/prepare_dataset.py --min-abs-words 60 --val-size 0.1 --test-size 0.1
```

**Output:**
- Raw data: `data/raw/arxiv_<timestamp>.jsonl`
- Processed data: `data/processed/papers_{train,val,test}.{jsonl,parquet}`

### Day 2: Fine-Tuning

```bash
# Step 1: Generate silver summaries
python scripts/generate_summaries.py --all

# Step 2: Filter papers with valid summaries
python scripts/filter_summaries.py

# Step 3: Train the model
python scripts/train_model.py

# Step 4: Test the model
python scripts/test_model.py

# Step 5: Evaluate (optional)
jupyter notebook notebooks/evaluate_model.ipynb
```

**Output:**
- Model checkpoints: `models/checkpoints/`
- Final model: `models/checkpoints/final_model/`
- Training logs: `logs/`
- W&B dashboard: `https://wandb.ai/<username>/paperpal`

### Training Configuration

Edit `config/training_config.yaml` to customize:
- Model selection (BART-base, BART-large, T5)
- Training hyperparameters (epochs, batch size, learning rate)
- Batch size and gradient accumulation
- Evaluation settings and metrics
- W&B project name and tags

**macOS Users:** The config is pre-configured for Apple Silicon compatibility with `dataloader_num_workers: 0`.

---

## ğŸ“ Project Structure

```
paperpal/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ training_config.yaml    # Training configuration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Raw arXiv downloads
â”‚   â”œâ”€â”€ interim/                 # Intermediate processing
â”‚   â””â”€â”€ processed/               # Final train/val/test splits
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cache/                   # Hugging Face model cache
â”‚   â””â”€â”€ checkpoints/             # Training checkpoints
â”‚       â””â”€â”€ final_model/         # âœ… Trained model ready!
â”‚
â”œâ”€â”€ logs/                        # TensorBoard logs
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ evaluate_model.ipynb     # Evaluation dashboard
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ paperpal/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py            # Configuration management
â”‚       â”œâ”€â”€ arxiv_client.py      # arXiv API client
â”‚       â”œâ”€â”€ data_prep.py         # Dataset preparation
â”‚       â”œâ”€â”€ silver_summaries.py  # Silver label generation
â”‚       â”œâ”€â”€ model.py             # Model wrapper
â”‚       â”œâ”€â”€ trainer.py           # Training pipeline
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ io.py            # I/O utilities
â”‚           â””â”€â”€ text.py          # Text processing
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ download_arxiv.py        # Download papers
    â”œâ”€â”€ prepare_dataset.py       # Prepare datasets
    â”œâ”€â”€ generate_summaries.py    # Generate silver summaries
    â”œâ”€â”€ filter_summaries.py      # Filter valid summaries
    â”œâ”€â”€ train_model.py           # Train model
    â””â”€â”€ test_model.py            # Test model
```

---

## ğŸ¯ Features

### Current (Days 1-2) âœ…

âœ… **Automated Data Pipeline**
- Query arXiv API with custom category and keyword filters
- Clean and preprocess abstracts (remove LaTeX, citations)
- Automatic train/val/test splitting with deduplication
- Both JSONL and Parquet format support

âœ… **Silver Summary Generation**
- Zero-shot summarization with pre-trained BART
- Multi-task: summaries, methods extraction, results extraction
- Quality validation and filtering (length, repetition checks)
- Batch processing with progress tracking

âœ… **Production-Ready Training**
- Hugging Face Trainer integration
- CPU-optimized (gradient accumulation, gradient checkpointing)
- Mixed precision training support (fp16 when GPU available)
- Early stopping and smart checkpointing
- Comprehensive logging (W&B + TensorBoard)
- macOS Apple Silicon compatible

âœ… **Evaluation & Testing**
- ROUGE score computation (ROUGE-1, ROUGE-2, ROUGE-L)
- Interactive testing script with sample papers
- Jupyter notebook for detailed analysis
- Example predictions and visualizations
- Compression ratio analysis

### Upcoming (Days 3-4) ğŸ”œ

ğŸ”œ **RAG Integration**
- FAISS vector database for semantic search
- Sentence-transformers embeddings (10,000+ papers)
- Context-aware summarization with retrieved papers
- Top-k retrieval with 90%+ accuracy

ğŸ”œ **Interactive Demo**
- Streamlit web interface
- FastAPI REST API endpoints
- Real-time paper summarization
- Paper recommendations based on similarity

---

## ğŸ“Š Monitoring & Logging

### Weights & Biases

Setup W&B for experiment tracking:

```bash
# Login to W&B
wandb login

# Your runs will appear at: https://wandb.ai/<username>/paperpal
```

Edit `config/training_config.yaml`:
```yaml
wandb:
  project: "paperpal"
  entity: "your-wandb-username"  # â† Add your username
  tags: ["bart-base", "summarization", "multi-task"]
```

### TensorBoard

```bash
# View training logs
tensorboard --logdir logs/
# Open: http://localhost:6006
```

---

## ğŸ“ Model Details

### Base Model
- **BART-base** (facebook/bart-base)
- 139M parameters
- Pre-trained on CNN/DailyMail summarization
- Bidirectional encoder + autoregressive decoder

### Fine-Tuning Approach
- **Task:** Abstractive summarization of research papers
- **Dataset:** arXiv papers (cs.CL, cs.LG, cs.AI, stat.ML)
- **Metrics:** ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum
- **Training:** 3 epochs, optimized for CPU
- **Technique:** Self-distillation with silver labels

### Multi-Task Learning
1. **Summarization:** Generate concise 30-80 word abstracts
2. **Method Extraction:** Identify research methods used
3. **Result Extraction:** Extract key findings and contributions

---

## ğŸ› ï¸ Advanced Usage

### Custom Model Training

```bash
# Train with different model
python scripts/train_model.py --model facebook/bart-large

# Adjust hyperparameters
python scripts/train_model.py --epochs 5 --batch-size 4 --learning-rate 5e-5

# Custom output directory
python scripts/train_model.py --output-dir ./custom_checkpoints

# Resume from checkpoint
python scripts/train_model.py --resume-from-checkpoint ./models/checkpoints/checkpoint-1000
```

### Interactive Testing

```bash
# Test with built-in sample papers (BERT, Transformer, GPT-3)
python scripts/test_model.py

# Interactive mode - paste your own abstracts
python scripts/test_model.py --interactive

# Test single abstract
python scripts/test_model.py --abstract "Your abstract here..."

# Test from file
python scripts/test_model.py --file path/to/abstract.txt
```

### Custom Data Collection

Edit `.env` to customize data collection:
```bash
# Target specific categories
ARXIV_CATEGORIES="cs.CL,cs.LG,cs.AI,stat.ML"

# Add keyword filters for focused results
ARXIV_FREE_TEXT="transformer OR attention OR BERT OR GPT OR language model"

# Collect more papers
ARXIV_MAX_RESULTS=500

# Filter by abstract length
MIN_ABS_WORDS=60
```

---

## ğŸ› Troubleshooting

### Common Issues

**1. NumPy Version Error**
```bash
# Fix: Downgrade to NumPy 1.x
pip install "numpy>=1.24.0,<2.0.0" --force-reinstall
```

**2. Python Version Error (`TypeError: unsupported operand type(s) for |`)**
```bash
# Fix: Upgrade to Python 3.10+
python3.11 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

**3. macOS MPS Multiprocessing Error**
```bash
# Fix: Already handled in config (dataloader_num_workers: 0)
# If you encounter issues, verify config/training_config.yaml has:
# dataloader_num_workers: 0
```

**4. arXiv API Returns 0 Results**
```bash
# Workaround: Use sample data or try different query
python scripts/download_arxiv.py --query "cat:cs.CL AND transformer"
```

**5. Missing Summaries During Training**
```bash
# Fix: Run the filter script
python scripts/filter_summaries.py
```

**6. Out of Memory**
```bash
# Fix: Reduce batch size in config/training_config.yaml
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
```

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- Additional model architectures (T5, Pegasus, LED)
- Better evaluation metrics (BERTScore, METEOR)
- Web scraping for non-arXiv papers
- Citation graph integration
- Multi-language support
- PDF processing pipeline

---

## ğŸ™ Acknowledgments

- Hugging Face for the Transformers library and model hub
- arXiv for providing open access to research papers
- BART authors (Lewis et al., 2019) for the base model
- Weights & Biases for experiment tracking tools
- The open-source ML community

---

## ğŸ“ Contact

- **Author:** Abhiram H Nambiar
- **GitHub:** [@ahnambia](https://github.com/ahnambia)
- **LinkedIn:** [abhiramnambiar](https://linkedin.com/in/abhiramnambiar/)
- **Project Link:** [https://github.com/ahnambia/PaperPal](https://github.com/ahnambia/PaperPal)

---

## ğŸ—ºï¸ Roadmap

- [x] **Day 1:** Data Collection & Preparation âœ…
  - [x] arXiv API integration
  - [x] Data cleaning and preprocessing
  - [x] Train/val/test splitting
  
- [x] **Day 2:** Fine-Tuning Setup âœ…
  - [x] Silver summary generation
  - [x] BART model fine-tuning
  - [x] Experiment tracking with W&B
  - [x] Model evaluation and testing
  
- [ ] **Day 3:** RAG Integration ğŸš§
  - [ ] FAISS index building
  - [ ] Semantic search implementation
  - [ ] Context-aware generation
  
- [ ] **Day 4:** Deployment & Demo ğŸš§
  - [ ] Streamlit UI
  - [ ] FastAPI backend
  - [ ] Hugging Face Space
  
- [ ] **Future Enhancements:**
  - [ ] Multi-modal support (PDFs, images)
  - [ ] Citation network analysis
  - [ ] Collaborative features
  - [ ] Real-time paper recommendations

---

## ğŸ† Project Stats

| Metric | Value |
|--------|-------|
| **Total Lines of Code** | ~2,000+ |
| **Model Parameters** | 139M |
| **Training Time** | 21 seconds (3 epochs) |
| **ROUGE-1 Score** | 100% |
| **Compression Ratio** | 1.1-1.2x |
| **Languages Used** | Python, YAML, Markdown |
| **Tools Integrated** | 10+ (PyTorch, Transformers, W&B, etc.) |

---