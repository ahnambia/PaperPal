# ğŸ“„ PaperPal â€” Research Paper Summarizer & Insight Extractor

**PaperPal** is an end-to-end NLP system that downloads research papers from **arXiv**, summarizes them using a fine-tuned transformer model, and extracts key insights such as methods and results.  
The project culminates in an interactive **Streamlit UI**, served via **FastAPI**, and exported as a **Hugging Face Space** demo.

---

## ğŸ§  Tech Stack

- **Core:** Python â€¢ Hugging Face Transformers â€¢ PyTorch  
- **Retrieval:** FAISS â€¢ RAG (Retrieval-Augmented Generation)  
- **Frontend & Serving:** Streamlit â€¢ FastAPI  
- **Utilities:** pandas â€¢ scikit-learn â€¢ dotenv â€¢ arxiv API
- **ML Tools:** Weights & Biases â€¢ TensorBoard â€¢ ROUGE metrics

---

## âœ… Progress

### Day 1: Data Collection & Preparation âœ…
- âœ… Repository scaffold with modular structure
- âœ… Configuration system (`.env` + `Config` class)
- âœ… arXiv downloader with rate-limiting
- âœ… Dataset preparation pipeline
- âœ… Train/val/test splits
- âœ… JSONL and Parquet outputs

### Day 2: Fine-Tuning Setup âœ…
- âœ… Silver summary generation with BART
- âœ… Multi-task learning (summaries, methods, results)
- âœ… Training pipeline with Hugging Face Trainer
- âœ… Weights & Biases integration
- âœ… CPU-optimized training
- âœ… Evaluation metrics (ROUGE)
- âœ… Model checkpointing and saving
- âœ… Evaluation notebook

### Day 3: RAG Integration (Upcoming)
- â³ FAISS index building
- â³ Semantic search with sentence-transformers
- â³ Context retrieval pipeline
- â³ RAG query interface

### Day 4: Deployment & Demo (Upcoming)
- â³ Streamlit UI
- â³ FastAPI backend
- â³ Hugging Face Space deployment
- â³ Docker containerization

---

## ğŸš€ Quickstart

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/ahnambia/PaperPal.git
cd PaperPal

# 2. Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Setup configuration
cp .env.example .env
# Edit .env to customize categories, keywords, etc.
```

### Day 1: Data Collection

```bash
# Download papers from arXiv
python scripts/download_arxiv.py --max-results 100

# Prepare and clean dataset
python scripts/prepare_dataset.py --min-abs-words 60 --val-size 0.1 --test-size 0.1
```

**Output:**
- Raw data: `data/raw/arxiv_<timestamp>.jsonl`
- Processed data: `data/processed/papers_{train,val,test}.{jsonl,parquet}`

### Day 2: Fine-Tuning

```bash
# Step 1: Generate silver summaries
python scripts/generate_summaries.py --all

# Step 2: Train the model
python scripts/train_model.py

# Step 3: Test the model
python scripts/test_model.py

# Step 4: Evaluate (optional)
# Open and run: notebooks/evaluate_model.ipynb
```

**Output:**
- Model checkpoints: `models/checkpoints/`
- Final model: `models/checkpoints/final_model/`
- Training logs: `logs/`
- W&B dashboard: Check your Weights & Biases project

### Training Configuration

Edit `config/training_config.yaml` to customize:
- Model selection (BART-base, BART-large, T5)
- Training hyperparameters
- Batch size and gradient accumulation
- Evaluation settings
- W&B project name

---

## ğŸ“ Project Structure

```
paperpal/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ training_config.yaml    # Training configuration
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Raw arXiv downloads
â”‚   â”œâ”€â”€ interim/                 # Intermediate processing
â”‚   â””â”€â”€ processed/               # Final train/val/test splits
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cache/                   # Hugging Face model cache
â”‚   â””â”€â”€ checkpoints/             # Training checkpoints
â”‚
â”œâ”€â”€ logs/                        # TensorBoard logs
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ evaluate_model.ipynb     # Evaluation dashboard
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ paperpal/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py            # Configuration management
â”‚       â”œâ”€â”€ arxiv_client.py      # arXiv API client
â”‚       â”œâ”€â”€ data_prep.py         # Dataset preparation
â”‚       â”œâ”€â”€ silver_summaries.py  # Silver label generation
â”‚       â”œâ”€â”€ model.py             # Model wrapper
â”‚       â”œâ”€â”€ trainer.py           # Training pipeline
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ io.py            # I/O utilities
â”‚           â””â”€â”€ text.py          # Text processing
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ download_arxiv.py        # Download papers
    â”œâ”€â”€ prepare_dataset.py       # Prepare datasets
    â”œâ”€â”€ generate_summaries.py    # Generate silver summaries
    â”œâ”€â”€ train_model.py           # Train model
    â””â”€â”€ test_model.py            # Test model
```

---

## ğŸ¯ Features

### Current (Days 1-2)

âœ… **Automated Data Pipeline**
- Query arXiv API with custom filters
- Clean and preprocess abstracts
- Automatic train/val/test splitting

âœ… **Silver Summary Generation**
- Zero-shot summarization with BART
- Multi-task: summaries, methods, results
- Quality validation and filtering

âœ… **Production-Ready Training**
- Hugging Face Trainer integration
- Mixed precision training (when GPU available)
- Gradient accumulation for larger effective batch sizes
- Early stopping and checkpointing
- Comprehensive logging (W&B + TensorBoard)

âœ… **Evaluation & Testing**
- ROUGE score computation
- Interactive testing script
- Jupyter notebook for analysis
- Example predictions and visualizations

### Upcoming (Days 3-4)

ğŸ”œ **RAG Integration**
- FAISS vector database
- Semantic paper search
- Context-aware summarization

ğŸ”œ **Interactive Demo**
- Streamlit web interface
- FastAPI REST API
- Real-time paper summarization
- Paper recommendations

---

## ğŸ“Š Monitoring & Logging

### Weights & Biases

Setup W&B for experiment tracking:

```bash
# Login to W&B
wandb login

# Your runs will appear at: https://wandb.ai/<username>/paperpal
```

Edit `config/training_config.yaml`:
```yaml
wandb:
  project: "paperpal"
  entity: "your-username"
  tags: ["bart-base", "summarization"]
```

### TensorBoard

```bash
# View training logs
tensorboard --logdir logs/
# Open: http://localhost:6006
```

---

## ğŸ“ Model Details

### Base Model
- **BART-base** (facebook/bart-base)
- 139M parameters
- Pre-trained on CNN/DailyMail summarization

### Fine-Tuning
- **Task:** Abstractive summarization of research papers
- **Dataset:** arXiv papers (cs.CL, cs.LG, cs.AI)
- **Metrics:** ROUGE-1, ROUGE-2, ROUGE-L
- **Training:** 3 epochs, ~2-4 hours on CPU

### Multi-Task Learning
1. **Summarization:** Generate concise abstracts
2. **Method Extraction:** Identify research methods
3. **Result Extraction:** Extract key findings

---

## ğŸ› ï¸ Advanced Usage

### Custom Model Training

```bash
# Train with different model
python scripts/train_model.py --model facebook/bart-large

# Adjust hyperparameters
python scripts/train_model.py --epochs 5 --batch-size 4 --learning-rate 5e-5

# Resume from checkpoint
python scripts/train_model.py --resume-from-checkpoint ./models/checkpoints/checkpoint-1000
```

### Interactive Testing

```bash
# Interactive mode - paste your own abstracts
python scripts/test_model.py --interactive

# Test single abstract
python scripts/test_model.py --abstract "Your abstract here..."

# Test from file
python scripts/test_model.py --file path/to/abstract.txt
```

### Custom Data Collection

Edit `.env` to customize data collection:
```bash
# Target specific categories
ARXIV_CATEGORIES="cs.CL,cs.LG,cs.AI,stat.ML"

# Add keyword filters
ARXIV_FREE_TEXT="transformer OR attention OR BERT OR GPT"

# Collect more papers
ARXIV_MAX_RESULTS=500
```

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- Additional model architectures (T5, Pegasus)
- Better evaluation metrics
- Web scraping for non-arXiv papers
- Citation graph integration
- Multi-language support

---

## ğŸ“ Citation

If you use PaperPal in your research, please cite:

```bibtex
@software{paperpal2025,
  author = {Your Name},
  title = {PaperPal: AI-Powered Research Paper Summarization},
  year = {2025},
  url = {https://github.com/ahnambia/PaperPal}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- Hugging Face for Transformers library
- arXiv for providing open access to research papers
- BART authors (Lewis et al., 2019)
- The open-source ML community

---

## ğŸ“ Contact

- GitHub: [@ahnambia](https://github.com/ahnambia)
- Project Link: [https://github.com/ahnambia/PaperPal](https://github.com/ahnambia/PaperPal)

---

## ğŸ—ºï¸ Roadmap

- [x] Day 1: Data Collection & Preparation
- [x] Day 2: Fine-Tuning Setup
- [ ] Day 3: RAG Integration
- [ ] Day 4: Deployment & Demo
- [ ] Future: Multi-modal support (PDFs, images)
- [ ] Future: Citation network analysis
- [ ] Future: Collaborative features

---

**Status:** âœ… Day 2 Complete â€” Fine-tuning pipeline ready!  
**Next:** Day 3 â€” RAG Integration ğŸš€
